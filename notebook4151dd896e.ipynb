{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.12.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":3111719,"sourceType":"datasetVersion","datasetId":1899282}],"dockerImageVersionId":31236,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"\"\"\"\nWaveFusion-Net: Dual-Branch Image Deblurring with Wavelet-Spatial Fusion\nHIDE Dataset Version (under 12h)\n\"\"\"\n\nimport os\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.utils.data import Dataset, DataLoader\nfrom torchvision import transforms, models\nfrom PIL import Image\nimport numpy as np\nfrom tqdm import tqdm\nfrom torch import amp\nimport matplotlib.pyplot as plt\nimport random\n\n# ======================== WAVELET TRANSFORMS ========================\nclass DWT(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.register_buffer('ll', torch.tensor([[0.5, 0.5], [0.5, 0.5]]).view(1, 1, 2, 2))\n        self.register_buffer('lh', torch.tensor([[0.5, 0.5], [-0.5, -0.5]]).view(1, 1, 2, 2))\n        self.register_buffer('hl', torch.tensor([[0.5, -0.5], [0.5, -0.5]]).view(1, 1, 2, 2))\n        self.register_buffer('hh', torch.tensor([[0.5, -0.5], [-0.5, 0.5]]).view(1, 1, 2, 2))\n\n    def forward(self, x):\n        B, C, H, W = x.shape\n        if H % 2 != 0:\n            x = F.pad(x, (0, 0, 0, 1))\n        if W % 2 != 0:\n            x = F.pad(x, (0, 1, 0, 0))\n        x = x.contiguous().view(B * C, 1, x.shape[2], x.shape[3])\n        ll = F.conv2d(x, self.ll, stride=2)\n        lh = F.conv2d(x, self.lh, stride=2)\n        hl = F.conv2d(x, self.hl, stride=2)\n        hh = F.conv2d(x, self.hh, stride=2)\n        return (\n            ll.view(B, C, ll.shape[2], ll.shape[3]),\n            lh.view(B, C, lh.shape[2], lh.shape[3]),\n            hl.view(B, C, hl.shape[2], hl.shape[3]),\n            hh.view(B, C, hh.shape[2], hh.shape[3]),\n        )\n\nclass IDWT(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.register_buffer('ll', torch.tensor([[0.5, 0.5], [0.5, 0.5]]).view(1, 1, 2, 2))\n        self.register_buffer('lh', torch.tensor([[0.5, 0.5], [-0.5, -0.5]]).view(1, 1, 2, 2))\n        self.register_buffer('hl', torch.tensor([[0.5, -0.5], [0.5, -0.5]]).view(1, 1, 2, 2))\n        self.register_buffer('hh', torch.tensor([[0.5, -0.5], [-0.5, 0.5]]).view(1, 1, 2, 2))\n\n    def forward(self, ll, lh, hl, hh):\n        B, C, H, W = ll.shape\n        ll = ll.view(B * C, 1, H, W)\n        lh = lh.view(B * C, 1, H, W)\n        hl = hl.view(B * C, 1, H, W)\n        hh = hh.view(B * C, 1, H, W)\n        ll = F.conv_transpose2d(ll, self.ll, stride=2)\n        lh = F.conv_transpose2d(lh, self.lh, stride=2)\n        hl = F.conv_transpose2d(hl, self.hl, stride=2)\n        hh = F.conv_transpose2d(hh, self.hh, stride=2)\n        out = ll + lh + hl + hh\n        return out.view(B, C, out.shape[2], out.shape[3])\n\n# ======================== ATTENTION MODULES ========================\nclass SimpleGate(nn.Module):\n    def forward(self, x):\n        x1, x2 = x.chunk(2, dim=1)\n        return x1 * x2\n\nclass StripAttention(nn.Module):\n    def __init__(self, channels, strip_size=7):\n        super().__init__()\n        self.h_conv = nn.Conv2d(channels, channels, (1, strip_size), padding=(0, strip_size // 2), groups=channels)\n        self.v_conv = nn.Conv2d(channels, channels, (strip_size, 1), padding=(strip_size // 2, 0), groups=channels)\n        self.proj = nn.Conv2d(channels * 2, channels, 1)\n\n    def forward(self, x):\n        B, C, H, W = x.shape\n        h_out = self.h_conv(x)[:, :, :H, :W]\n        v_out = self.v_conv(x)[:, :, :H, :W]\n        h_attn = torch.sigmoid(h_out)\n        v_attn = torch.sigmoid(v_out)\n        return self.proj(torch.cat([x * h_attn, x * v_attn], dim=1))\n\nclass SCA(nn.Module):\n    def __init__(self, channels):\n        super().__init__()\n        self.gap = nn.AdaptiveAvgPool2d(1)\n        self.fc = nn.Conv2d(channels, channels, 1)\n\n    def forward(self, x):\n        return x * self.fc(self.gap(x))\n\n# ======================== CORE BLOCKS ========================\nclass NAFBlock(nn.Module):\n    def __init__(self, channels, dw_expand=2):\n        super().__init__()\n        dw_channels = channels * dw_expand\n        self.conv1 = nn.Conv2d(channels, dw_channels, 1)\n        self.conv2 = nn.Conv2d(dw_channels, dw_channels, 3, padding=1, groups=dw_channels)\n        self.conv3 = nn.Conv2d(dw_channels // 2, channels, 1)\n        self.sca = SCA(dw_channels // 2)\n        self.sg = SimpleGate()\n        self.norm = nn.LayerNorm(channels)\n\n    def forward(self, x):\n        residual = x\n        x = x.permute(0, 2, 3, 1)\n        x = self.norm(x)\n        x = x.permute(0, 3, 1, 2)\n        x = self.conv1(x)\n        x = self.conv2(x)\n        x = self.sg(x)\n        x = self.sca(x)\n        x = self.conv3(x)\n        return x + residual\n\nclass WaveletBlock(nn.Module):\n    def __init__(self, channels):\n        super().__init__()\n        self.ll_conv = nn.Sequential(\n            nn.Conv2d(channels, channels, 3, padding=1),\n            nn.GELU(),\n            nn.Conv2d(channels, channels, 3, padding=1)\n        )\n        self.hf_conv = nn.Sequential(\n            nn.Conv2d(channels * 3, channels * 3, 3, padding=1, groups=3),\n            nn.GELU(),\n            nn.Conv2d(channels * 3, channels * 3, 3, padding=1, groups=3)\n        )\n        self.hf_attn = nn.Sequential(\n            nn.AdaptiveAvgPool2d(1),\n            nn.Conv2d(channels * 3, channels * 3, 1),\n            nn.Sigmoid()\n        )\n\n    def forward(self, ll, lh, hl, hh):\n        ll_out = ll + self.ll_conv(ll)\n        hf = torch.cat([lh, hl, hh], dim=1)\n        hf_feat = self.hf_conv(hf)\n        hf_attn = self.hf_attn(hf_feat)\n        hf_out = hf + hf_feat * hf_attn\n        lh_out, hl_out, hh_out = hf_out.chunk(3, dim=1)\n        return ll_out, lh_out, hl_out, hh_out\n\nclass CrossBranchFusion(nn.Module):\n    def __init__(self, channels):\n        super().__init__()\n        self.spatial_proj = nn.Conv2d(channels, channels, 1)\n        self.wavelet_proj = nn.Conv2d(channels, channels, 1)\n        self.gate = nn.Sequential(\n            nn.Conv2d(channels * 2, channels, 1),\n            nn.Sigmoid()\n        )\n        self.out = nn.Conv2d(channels, channels, 1)\n\n    def forward(self, spatial_feat, wavelet_feat):\n        if spatial_feat.shape[2:] != wavelet_feat.shape[2:]:\n            wavelet_feat = F.interpolate(wavelet_feat, size=spatial_feat.shape[2:], mode='bilinear', align_corners=False)\n        s = self.spatial_proj(spatial_feat)\n        w = self.wavelet_proj(wavelet_feat)\n        gate = self.gate(torch.cat([s, w], dim=1))\n        fused = gate * s + (1 - gate) * w\n        return self.out(fused)\n\n# ======================== MAIN NETWORK ========================\nclass WaveFusionNet(nn.Module):\n    def __init__(self, in_channels=3, out_channels=3, base_channels=48, num_blocks=[4, 6, 6, 4]):\n        super().__init__()\n        self.dwt = DWT()\n        self.idwt = IDWT()\n        self.intro = nn.Conv2d(in_channels, base_channels, 3, padding=1)\n        self.enc1 = nn.Sequential(*[NAFBlock(base_channels) for _ in range(num_blocks[0])])\n        self.down1 = nn.Conv2d(base_channels, base_channels * 2, 2, stride=2)\n        self.enc2 = nn.Sequential(*[NAFBlock(base_channels * 2) for _ in range(num_blocks[1])])\n        self.down2 = nn.Conv2d(base_channels * 2, base_channels * 4, 2, stride=2)\n        self.enc3 = nn.Sequential(*[NAFBlock(base_channels * 4) for _ in range(num_blocks[2])])\n        self.down3 = nn.Conv2d(base_channels * 4, base_channels * 8, 2, stride=2)\n        self.wav_intro = nn.Conv2d(in_channels, base_channels, 3, padding=1)\n        self.wav_block1 = WaveletBlock(base_channels)\n        self.wav_proj1 = nn.Conv2d(base_channels, base_channels * 2, 1)\n        self.wav_block2 = WaveletBlock(base_channels * 2)\n        self.wav_proj2 = nn.Conv2d(base_channels * 2, base_channels * 4, 1)\n        self.wav_block3 = WaveletBlock(base_channels * 4)\n        self.fusion1 = CrossBranchFusion(base_channels * 2)\n        self.fusion2 = CrossBranchFusion(base_channels * 4)\n        self.bottleneck = nn.Sequential(\n            NAFBlock(base_channels * 8),\n            StripAttention(base_channels * 8),\n            NAFBlock(base_channels * 8),\n            NAFBlock(base_channels * 8),\n        )\n        self.up3 = nn.ConvTranspose2d(base_channels * 8, base_channels * 4, 2, stride=2)\n        self.dec3 = nn.Sequential(*[NAFBlock(base_channels * 4) for _ in range(num_blocks[2])])\n        self.up2 = nn.ConvTranspose2d(base_channels * 4, base_channels * 2, 2, stride=2)\n        self.dec2 = nn.Sequential(*[NAFBlock(base_channels * 2) for _ in range(num_blocks[1])])\n        self.up1 = nn.ConvTranspose2d(base_channels * 2, base_channels, 2, stride=2)\n        self.dec1 = nn.Sequential(*[NAFBlock(base_channels) for _ in range(num_blocks[0])])\n        self.refine = nn.Sequential(\n            nn.Conv2d(base_channels, base_channels, 3, padding=1),\n            nn.GELU(),\n            nn.Conv2d(base_channels, base_channels, 3, padding=1),\n        )\n        self.outro = nn.Conv2d(base_channels, out_channels, 3, padding=1)\n\n    def forward(self, x):\n        B, C, H, W = x.shape\n        pad_h = (8 - H % 8) % 8\n        pad_w = (8 - W % 8) % 8\n        if pad_h > 0 or pad_w > 0:\n            x = F.pad(x, (0, pad_w, 0, pad_h), mode='reflect')\n        f0 = self.intro(x)\n        f1 = self.enc1(f0)\n        f1_down = self.down1(f1)\n        f2 = self.enc2(f1_down)\n        f2_down = self.down2(f2)\n        f3 = self.enc3(f2_down)\n        f3_down = self.down3(f3)\n        w0 = self.wav_intro(x)\n        ll1, lh1, hl1, hh1 = self.dwt(w0)\n        ll1, lh1, hl1, hh1 = self.wav_block1(ll1, lh1, hl1, hh1)\n        w1 = self.wav_proj1(ll1)\n        ll2, lh2, hl2, hh2 = self.dwt(w1)\n        ll2, lh2, hl2, hh2 = self.wav_block2(ll2, lh2, hl2, hh2)\n        w2 = self.wav_proj2(ll2)\n        ll3, lh3, hl3, hh3 = self.dwt(w2)\n        ll3, lh3, hl3, hh3 = self.wav_block3(ll3, lh3, hl3, hh3)\n        f2_fused = self.fusion1(f2, w1)\n        f3_fused = self.fusion2(f3, w2)\n        bottleneck_out = self.bottleneck(f3_down)\n        d3 = self.up3(bottleneck_out) + f3_fused\n        d3 = self.dec3(d3)\n        d2 = self.up2(d3) + f2_fused\n        d2 = self.dec2(d2)\n        d1 = self.up1(d2) + f1\n        d1 = self.dec1(d1)\n        out = self.refine(d1)\n        out = out + f0\n        out = self.outro(out)\n        out = out + x\n        if pad_h > 0 or pad_w > 0:\n            out = out[:, :, :H, :W]\n        return out\n\n# ======================== LOSS FUNCTIONS ========================\nclass VGGPerceptualLoss(nn.Module):\n    def __init__(self):\n        super().__init__()\n        vgg = models.vgg19(weights=models.VGG19_Weights.IMAGENET1K_V1).features\n        self.slice1 = nn.Sequential(*list(vgg.children())[:4])\n        self.slice2 = nn.Sequential(*list(vgg.children())[4:9])\n        self.slice3 = nn.Sequential(*list(vgg.children())[9:18])\n        for p in self.parameters():\n            p.requires_grad = False\n        self.register_buffer('mean', torch.tensor([0.485, 0.456, 0.406]).view(1, 3, 1, 1))\n        self.register_buffer('std', torch.tensor([0.229, 0.224, 0.225]).view(1, 3, 1, 1))\n\n    def forward(self, pred, target):\n        pred = (pred - self.mean) / self.std\n        target = (target - self.mean) / self.std\n        pf1 = self.slice1(pred)\n        pf2 = self.slice2(pf1)\n        pf3 = self.slice3(pf2)\n        with torch.no_grad():\n            tf1 = self.slice1(target)\n            tf2 = self.slice2(tf1)\n            tf3 = self.slice3(tf2)\n        return F.l1_loss(pf1, tf1) + F.l1_loss(pf2, tf2) + F.l1_loss(pf3, tf3)\n\nclass FFTLoss(nn.Module):\n    def forward(self, pred, target):\n        pred_fft = torch.fft.rfft2(pred)\n        target_fft = torch.fft.rfft2(target)\n        return F.l1_loss(pred_fft.real, target_fft.real) + F.l1_loss(pred_fft.imag, target_fft.imag)\n\nclass GradientLoss(nn.Module):\n    def __init__(self):\n        super().__init__()\n        sobel_x = torch.tensor([[-1, 0, 1], [-2, 0, 2], [-1, 0, 1]], dtype=torch.float32)\n        sobel_y = torch.tensor([[-1, -2, -1], [0, 0, 0], [1, 2, 1]], dtype=torch.float32)\n        self.register_buffer('sobel_x', sobel_x.view(1, 1, 3, 3).repeat(3, 1, 1, 1))\n        self.register_buffer('sobel_y', sobel_y.view(1, 1, 3, 3).repeat(3, 1, 1, 1))\n\n    def forward(self, pred, target):\n        pgx = F.conv2d(pred, self.sobel_x, padding=1, groups=3)\n        pgy = F.conv2d(pred, self.sobel_y, padding=1, groups=3)\n        tgx = F.conv2d(target, self.sobel_x, padding=1, groups=3)\n        tgy = F.conv2d(target, self.sobel_y, padding=1, groups=3)\n        return F.l1_loss(pgx, tgx) + F.l1_loss(pgy, tgy)\n\nclass WaveletHFLoss(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.dwt = DWT()\n\n    def forward(self, pred, target):\n        _, plh, phl, phh = self.dwt(pred)\n        _, tlh, thl, thh = self.dwt(target)\n        return F.l1_loss(plh, tlh) + F.l1_loss(phl, thl) + F.l1_loss(phh, thh)\n\nclass CombinedLoss(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.l1 = nn.L1Loss()\n        self.vgg = VGGPerceptualLoss()\n        self.fft = FFTLoss()\n        self.gradient = GradientLoss()\n        self.wavelet_hf = WaveletHFLoss()\n        self.w_l1 = 1.0\n        self.w_vgg = 0.1\n        self.w_fft = 0.05\n        self.w_gradient = 0.1\n        self.w_wavelet = 0.02\n\n    def forward(self, pred, target):\n        l1_loss = self.l1(pred, target)\n        vgg_loss = self.vgg(pred, target)\n        fft_loss = self.fft(pred, target)\n        gradient_loss = self.gradient(pred, target)\n        wavelet_loss = self.wavelet_hf(pred, target)\n        total = (self.w_l1 * l1_loss + self.w_vgg * vgg_loss + self.w_fft * fft_loss +\n                 self.w_gradient * gradient_loss + self.w_wavelet * wavelet_loss)\n        return total, {\n            'l1': l1_loss.item(),\n            'vgg': vgg_loss.item(),\n            'fft': fft_loss.item(),\n            'gradient': gradient_loss.item(),\n            'wavelet': wavelet_loss.item(),\n        }\n\n# ======================== DATASET ========================\nclass HIDEPairs(Dataset):\n    \"\"\"HIDE dataset loader. Supports blur/GT folders or train/test txt lists.\"\"\"\n    def __init__(self, root_dir, split='train', patch_size=256):\n        self.patch_size = patch_size\n        self.split = split\n        self.blur_images = []\n        self.sharp_images = []\n\n        def resolve(path_str):\n            # Honor absolute paths from txt; otherwise treat as relative to root_dir\n            if os.path.isabs(path_str) or path_str.startswith('/'):\n                return path_str\n            return os.path.join(root_dir, path_str)\n\n        list_path = os.path.join(root_dir, f\"{split}.txt\")\n        used_txt = False\n        if os.path.exists(list_path):\n            used_txt = True\n            with open(list_path, 'r') as f:\n                for line in f:\n                    parts = line.strip().split()\n                    if len(parts) >= 2:\n                        blur_path = resolve(parts[0])\n                        gt_path = resolve(parts[1])\n                        if os.path.exists(blur_path) and os.path.exists(gt_path):\n                            self.blur_images.append(blur_path)\n                            self.sharp_images.append(gt_path)\n                    elif len(parts) == 1 and parts[0]:\n                        blur_path = resolve(parts[0])\n                        fname = os.path.basename(blur_path)\n                        candidates = [\n                            os.path.join(root_dir, 'GT', fname),\n                            os.path.join(root_dir, 'gt', fname),\n                            os.path.join(root_dir, split, 'GT', fname),\n                            os.path.join(root_dir, split, 'gt', fname),\n                        ]\n                        gt_path = next((p for p in candidates if os.path.exists(p)), None)\n                        if os.path.exists(blur_path) and gt_path:\n                            self.blur_images.append(blur_path)\n                            self.sharp_images.append(gt_path)\n        if not used_txt or (used_txt and len(self.blur_images) == 0):\n            blur_dir_candidates = [\n                os.path.join(root_dir, split, 'blur'),\n                os.path.join(root_dir, split, 'input'),\n                os.path.join(root_dir, split),\n                os.path.join(root_dir, 'blur'),\n                os.path.join(root_dir, 'input'),\n                os.path.join(root_dir, split, 'test'),\n                os.path.join(root_dir, 'test'),\n            ]\n            gt_dir_candidates = [\n                os.path.join(root_dir, split, 'gt'),\n                os.path.join(root_dir, split, 'GT'),\n                os.path.join(root_dir, 'gt'),\n                os.path.join(root_dir, 'GT'),\n            ]\n            gt_dir = next((p for p in gt_dir_candidates if os.path.exists(p)), None)\n\n            # Scan recursively for blur files\n            extensions = ('.png', '.jpg', '.jpeg')\n            for blur_root in blur_dir_candidates:\n                if not os.path.exists(blur_root):\n                    continue\n                for dirpath, _, filenames in os.walk(blur_root):\n                    # skip GT folders\n                    if 'GT' in os.path.normpath(dirpath).split(os.sep):\n                        continue\n                    for name in filenames:\n                        if not name.lower().endswith(extensions):\n                            continue\n                        blur_path = os.path.join(dirpath, name)\n                        if gt_dir:\n                            gt_path = os.path.join(gt_dir, name)\n                        else:\n                            gt_path = None\n                            for candidate in [\n                                os.path.join(root_dir, 'GT', name),\n                                os.path.join(root_dir, 'gt', name),\n                            ]:\n                                if os.path.exists(candidate):\n                                    gt_path = candidate\n                                    break\n                        if gt_path and os.path.exists(gt_path):\n                            self.blur_images.append(blur_path)\n                            self.sharp_images.append(gt_path)\n        print(f\"Found {len(self.blur_images)} {split} pairs in HIDE\")\n        if len(self.blur_images) == 0:\n            raise ValueError(f\"No {split} pairs found in HIDE. Check train/test.txt or folder structure.\")\n        self.to_tensor = transforms.ToTensor()\n\n    def __len__(self):\n        return len(self.blur_images)\n\n    def __getitem__(self, idx):\n        blur = Image.open(self.blur_images[idx]).convert('RGB')\n        sharp = Image.open(self.sharp_images[idx]).convert('RGB')\n        blur = self.to_tensor(blur)\n        sharp = self.to_tensor(sharp)\n        if self.split == 'train' and self.patch_size:\n            _, h, w = blur.shape\n            if h >= self.patch_size and w >= self.patch_size:\n                top = np.random.randint(0, h - self.patch_size + 1)\n                left = np.random.randint(0, w - self.patch_size + 1)\n                blur = blur[:, top:top + self.patch_size, left:left + self.patch_size]\n                sharp = sharp[:, top:top + self.patch_size, left:left + self.patch_size]\n            if np.random.random() > 0.5:\n                blur = torch.flip(blur, [2])\n                sharp = torch.flip(sharp, [2])\n            if np.random.random() > 0.5:\n                blur = torch.flip(blur, [1])\n                sharp = torch.flip(sharp, [1])\n        return blur, sharp\n\n# ======================== METRICS ========================\ndef calculate_psnr(pred, target):\n    mse = F.mse_loss(pred, target)\n    if mse == 0:\n        return float('inf')\n    return 10 * torch.log10(1.0 / mse)\n\n# ======================== VISUALIZATION ========================\ndef visualize_results(model, test_loader, device, save_dir, num_samples=8):\n    model.eval()\n    results_dir = os.path.join(save_dir, 'results_hide')\n    os.makedirs(results_dir, exist_ok=True)\n    total = len(test_loader.dataset)\n    indices = random.sample(range(total), min(num_samples, total))\n    stats = []\n    with torch.no_grad():\n        for idx, sample_idx in enumerate(indices):\n            blur, sharp = test_loader.dataset[sample_idx]\n            blur = blur.unsqueeze(0).to(device)\n            sharp = sharp.unsqueeze(0).to(device)\n            with amp.autocast('cuda'):\n                pred = model(blur)\n            pred = torch.clamp(pred, 0, 1)\n            psnr = calculate_psnr(pred, sharp).item()\n            stats.append(psnr)\n            comparison = torch.cat([blur.cpu(), pred.cpu(), sharp.cpu()], dim=0)\n            grid = torch.cat([comparison[i] for i in range(3)], dim=2)\n            img = transforms.ToPILImage()(grid)\n            img.save(os.path.join(results_dir, f'sample_{idx}_psnr{psnr:.2f}.png'))\n    print(f\"Saved {len(indices)} comparisons to {results_dir}; mean PSNR={np.mean(stats):.2f}dB\")\n\n# ======================== TRAINING ========================\ndef train():\n    config = {\n        'data_root': '/kaggle/input/hideblur/HIDE_dataset',  # adjust to your path\n        'batch_size': 4,\n        'patch_size': 256,\n        'epochs': 30,\n        'lr': 2e-4,\n        'min_lr': 1e-7,\n        'num_workers': 4,\n        'base_channels': 48,\n        'num_blocks': [4, 6, 6, 4],\n        'save_dir': '/kaggle/working',\n    }\n\n    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n    print(f\"Using device: {device}\")\n    # Build model\n    model = WaveFusionNet(base_channels=config['base_channels'], num_blocks=config['num_blocks'])\n\n    # Print detailed architecture table\n    print(\"\\n\" + \"=\"*80)\n    print(\"MODEL ARCHITECTURE: WaveFusion-Net (HIDE)\")\n    print(\"=\"*80)\n\n    total_params = 0\n    trainable_params = 0\n    print(f\"{'Module':<40} {'Parameters':<15} {'Shape':<25}\")\n    print(\"-\"*80)\n    for name, param in model.named_parameters():\n        params = param.numel()\n        total_params += params\n        if param.requires_grad:\n            trainable_params += params\n        short_name = name.replace('module.', '')\n        print(f\"{short_name:<40} {params:>12,} {str(list(param.shape)):<25}\")\n    print(\"-\"*80)\n    print(f\"{'Total Parameters':<40} {total_params:>12,}\")\n    print(f\"{'Trainable Parameters':<40} {trainable_params:>12,}\")\n    print(f\"{'Total (Millions)':<40} {total_params/1e6:>12.2f}M\")\n    print(\"=\"*80)\n\n    # Architecture summary\n    print(\"\\nARCHITECTURE SUMMARY:\")\n    print(f\"  - Base Channels: {config['base_channels']}\")\n    print(f\"  - Encoder Blocks: {config['num_blocks']}\")\n    print(f\"  - Dual-Branch: Spatial (NAFBlocks) + Wavelet (DWT)\")\n    print(f\"  - Fusion: Cross-Branch Gated Fusion at 2 levels\")\n    print(f\"  - Bottleneck: Strip Attention\")\n    print(\"=\"*80 + \"\\n\")\n\n    if torch.cuda.device_count() > 1:\n        print(f\"Using {torch.cuda.device_count()} GPUs\")\n        model = nn.DataParallel(model)\n    model = model.to(device)\n\n    criterion = CombinedLoss().to(device)\n    optimizer = torch.optim.AdamW(model.parameters(), lr=config['lr'], weight_decay=1e-4)\n    scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=config['epochs'], eta_min=config['min_lr'])\n    scaler = amp.GradScaler('cuda')\n\n    train_dataset = HIDEPairs(config['data_root'], split='train', patch_size=config['patch_size'])\n    test_dataset = HIDEPairs(config['data_root'], split='test', patch_size=None)\n    train_loader = DataLoader(train_dataset, batch_size=config['batch_size'], shuffle=True, num_workers=config['num_workers'], pin_memory=True, drop_last=True)\n    test_loader = DataLoader(test_dataset, batch_size=1, shuffle=False, num_workers=2)\n\n    # Baseline PSNR check (fewer samples for speed)\n    print(\"\\n=== Baseline Check ===\")\n    with torch.no_grad():\n        baseline_psnrs = []\n        for i, (blur, sharp) in enumerate(test_loader):\n            if i >= 5:\n                break\n            baseline_psnrs.append(calculate_psnr(blur, sharp).item())\n        if len(baseline_psnrs) > 0:\n            print(f\"Baseline PSNR (blur vs sharp): {np.mean(baseline_psnrs):.2f} dB\")\n        else:\n            print(\"Baseline PSNR: no test samples found.\")\n\n    best_psnr = 0\n    print(\"\\n=== Starting Training ===\")\n    for epoch in range(config['epochs']):\n        model.train()\n        epoch_loss = 0\n        loss_components = {'l1': 0, 'vgg': 0, 'fft': 0, 'gradient': 0, 'wavelet': 0}\n        pbar = tqdm(train_loader, desc=f\"Epoch {epoch+1}/{config['epochs']}\")\n        for blur, sharp in pbar:\n            blur = blur.to(device)\n            sharp = sharp.to(device)\n            optimizer.zero_grad()\n            with amp.autocast('cuda'):\n                pred = model(blur)\n                loss, comps = criterion(pred, sharp)\n            scaler.scale(loss).backward()\n            scaler.unscale_(optimizer)\n            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n            scaler.step(optimizer)\n            scaler.update()\n            epoch_loss += loss.item()\n            for k, v in comps.items():\n                loss_components[k] += v\n            pbar.set_postfix({'loss': f\"{loss.item():.4f}\", 'lr': f\"{optimizer.param_groups[0]['lr']:.2e}\"})\n        scheduler.step()\n        n_batches = len(train_loader)\n        epoch_loss /= n_batches\n        for k in loss_components:\n            loss_components[k] /= n_batches\n\n        if (epoch + 1) % 10 == 0 or epoch == 0:\n            model.eval()\n            val_psnr = 0\n            with torch.no_grad():\n                for blur, sharp in tqdm(test_loader, desc=\"Validating\"):\n                    blur = blur.to(device)\n                    sharp = sharp.to(device)\n                    with amp.autocast('cuda'):\n                        pred = model(blur)\n                    pred = torch.clamp(pred, 0, 1)\n                    val_psnr += calculate_psnr(pred, sharp).item()\n            val_psnr /= len(test_loader)\n            print(f\"Epoch {epoch+1}: Loss={epoch_loss:.4f}, PSNR={val_psnr:.2f}dB\")\n            print(f\"  Components - L1:{loss_components['l1']:.4f}, VGG:{loss_components['vgg']:.4f}, FFT:{loss_components['fft']:.4f}, Grad:{loss_components['gradient']:.4f}, Wav:{loss_components['wavelet']:.4f}\")\n            if val_psnr > best_psnr:\n                best_psnr = val_psnr\n                save_path = os.path.join(config['save_dir'], 'best_model_hide.pth')\n                state_dict = model.module.state_dict() if hasattr(model, 'module') else model.state_dict()\n                torch.save({'epoch': epoch + 1, 'model_state_dict': state_dict, 'optimizer_state_dict': optimizer.state_dict(), 'psnr': val_psnr}, save_path)\n                print(f\"  *** New best model saved! PSNR: {val_psnr:.2f}dB ***\")\n\n        if (epoch + 1) % 20 == 0:\n            save_path = os.path.join(config['save_dir'], f'checkpoint_hide_epoch{epoch+1}.pth')\n            state_dict = model.module.state_dict() if hasattr(model, 'module') else model.state_dict()\n            torch.save({'epoch': epoch + 1, 'model_state_dict': state_dict, 'optimizer_state_dict': optimizer.state_dict(), 'scheduler_state_dict': scheduler.state_dict()}, save_path)\n\n    print(f\"\\n=== Training Complete ===\\nBest PSNR: {best_psnr:.2f}dB\")\n    if best_psnr > 0:\n        best_model_path = os.path.join(config['save_dir'], 'best_model_hide.pth')\n        if os.path.exists(best_model_path):\n            checkpoint = torch.load(best_model_path)\n            if hasattr(model, 'module'):\n                model.module.load_state_dict(checkpoint['model_state_dict'])\n            else:\n                model.load_state_dict(checkpoint['model_state_dict'])\n            visualize_results(model, test_loader, device, config['save_dir'], num_samples=8)\n\nif __name__ == '__main__':\n    train()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-14T06:09:38.066225Z","iopub.execute_input":"2026-01-14T06:09:38.066488Z","iopub.status.idle":"2026-01-14T15:36:59.676689Z","shell.execute_reply.started":"2026-01-14T06:09:38.066460Z","shell.execute_reply":"2026-01-14T15:36:59.675934Z"}},"outputs":[{"name":"stdout","text":"Using device: cuda\n\n================================================================================\nMODEL ARCHITECTURE: WaveFusion-Net (HIDE)\n================================================================================\nModule                                   Parameters      Shape                    \n--------------------------------------------------------------------------------\nintro.weight                                    1,296 [48, 3, 3, 3]            \nintro.bias                                         48 [48]                     \nenc1.0.conv1.weight                             4,608 [96, 48, 1, 1]           \nenc1.0.conv1.bias                                  96 [96]                     \nenc1.0.conv2.weight                               864 [96, 1, 3, 3]            \nenc1.0.conv2.bias                                  96 [96]                     \nenc1.0.conv3.weight                             2,304 [48, 48, 1, 1]           \nenc1.0.conv3.bias                                  48 [48]                     \nenc1.0.sca.fc.weight                            2,304 [48, 48, 1, 1]           \nenc1.0.sca.fc.bias                                 48 [48]                     \nenc1.0.norm.weight                                 48 [48]                     \nenc1.0.norm.bias                                   48 [48]                     \nenc1.1.conv1.weight                             4,608 [96, 48, 1, 1]           \nenc1.1.conv1.bias                                  96 [96]                     \nenc1.1.conv2.weight                               864 [96, 1, 3, 3]            \nenc1.1.conv2.bias                                  96 [96]                     \nenc1.1.conv3.weight                             2,304 [48, 48, 1, 1]           \nenc1.1.conv3.bias                                  48 [48]                     \nenc1.1.sca.fc.weight                            2,304 [48, 48, 1, 1]           \nenc1.1.sca.fc.bias                                 48 [48]                     \nenc1.1.norm.weight                                 48 [48]                     \nenc1.1.norm.bias                                   48 [48]                     \nenc1.2.conv1.weight                             4,608 [96, 48, 1, 1]           \nenc1.2.conv1.bias                                  96 [96]                     \nenc1.2.conv2.weight                               864 [96, 1, 3, 3]            \nenc1.2.conv2.bias                                  96 [96]                     \nenc1.2.conv3.weight                             2,304 [48, 48, 1, 1]           \nenc1.2.conv3.bias                                  48 [48]                     \nenc1.2.sca.fc.weight                            2,304 [48, 48, 1, 1]           \nenc1.2.sca.fc.bias                                 48 [48]                     \nenc1.2.norm.weight                                 48 [48]                     \nenc1.2.norm.bias                                   48 [48]                     \nenc1.3.conv1.weight                             4,608 [96, 48, 1, 1]           \nenc1.3.conv1.bias                                  96 [96]                     \nenc1.3.conv2.weight                               864 [96, 1, 3, 3]            \nenc1.3.conv2.bias                                  96 [96]                     \nenc1.3.conv3.weight                             2,304 [48, 48, 1, 1]           \nenc1.3.conv3.bias                                  48 [48]                     \nenc1.3.sca.fc.weight                            2,304 [48, 48, 1, 1]           \nenc1.3.sca.fc.bias                                 48 [48]                     \nenc1.3.norm.weight                                 48 [48]                     \nenc1.3.norm.bias                                   48 [48]                     \ndown1.weight                                   18,432 [96, 48, 2, 2]           \ndown1.bias                                         96 [96]                     \nenc2.0.conv1.weight                            18,432 [192, 96, 1, 1]          \nenc2.0.conv1.bias                                 192 [192]                    \nenc2.0.conv2.weight                             1,728 [192, 1, 3, 3]           \nenc2.0.conv2.bias                                 192 [192]                    \nenc2.0.conv3.weight                             9,216 [96, 96, 1, 1]           \nenc2.0.conv3.bias                                  96 [96]                     \nenc2.0.sca.fc.weight                            9,216 [96, 96, 1, 1]           \nenc2.0.sca.fc.bias                                 96 [96]                     \nenc2.0.norm.weight                                 96 [96]                     \nenc2.0.norm.bias                                   96 [96]                     \nenc2.1.conv1.weight                            18,432 [192, 96, 1, 1]          \nenc2.1.conv1.bias                                 192 [192]                    \nenc2.1.conv2.weight                             1,728 [192, 1, 3, 3]           \nenc2.1.conv2.bias                                 192 [192]                    \nenc2.1.conv3.weight                             9,216 [96, 96, 1, 1]           \nenc2.1.conv3.bias                                  96 [96]                     \nenc2.1.sca.fc.weight                            9,216 [96, 96, 1, 1]           \nenc2.1.sca.fc.bias                                 96 [96]                     \nenc2.1.norm.weight                                 96 [96]                     \nenc2.1.norm.bias                                   96 [96]                     \nenc2.2.conv1.weight                            18,432 [192, 96, 1, 1]          \nenc2.2.conv1.bias                                 192 [192]                    \nenc2.2.conv2.weight                             1,728 [192, 1, 3, 3]           \nenc2.2.conv2.bias                                 192 [192]                    \nenc2.2.conv3.weight                             9,216 [96, 96, 1, 1]           \nenc2.2.conv3.bias                                  96 [96]                     \nenc2.2.sca.fc.weight                            9,216 [96, 96, 1, 1]           \nenc2.2.sca.fc.bias                                 96 [96]                     \nenc2.2.norm.weight                                 96 [96]                     \nenc2.2.norm.bias                                   96 [96]                     \nenc2.3.conv1.weight                            18,432 [192, 96, 1, 1]          \nenc2.3.conv1.bias                                 192 [192]                    \nenc2.3.conv2.weight                             1,728 [192, 1, 3, 3]           \nenc2.3.conv2.bias                                 192 [192]                    \nenc2.3.conv3.weight                             9,216 [96, 96, 1, 1]           \nenc2.3.conv3.bias                                  96 [96]                     \nenc2.3.sca.fc.weight                            9,216 [96, 96, 1, 1]           \nenc2.3.sca.fc.bias                                 96 [96]                     \nenc2.3.norm.weight                                 96 [96]                     \nenc2.3.norm.bias                                   96 [96]                     \nenc2.4.conv1.weight                            18,432 [192, 96, 1, 1]          \nenc2.4.conv1.bias                                 192 [192]                    \nenc2.4.conv2.weight                             1,728 [192, 1, 3, 3]           \nenc2.4.conv2.bias                                 192 [192]                    \nenc2.4.conv3.weight                             9,216 [96, 96, 1, 1]           \nenc2.4.conv3.bias                                  96 [96]                     \nenc2.4.sca.fc.weight                            9,216 [96, 96, 1, 1]           \nenc2.4.sca.fc.bias                                 96 [96]                     \nenc2.4.norm.weight                                 96 [96]                     \nenc2.4.norm.bias                                   96 [96]                     \nenc2.5.conv1.weight                            18,432 [192, 96, 1, 1]          \nenc2.5.conv1.bias                                 192 [192]                    \nenc2.5.conv2.weight                             1,728 [192, 1, 3, 3]           \nenc2.5.conv2.bias                                 192 [192]                    \nenc2.5.conv3.weight                             9,216 [96, 96, 1, 1]           \nenc2.5.conv3.bias                                  96 [96]                     \nenc2.5.sca.fc.weight                            9,216 [96, 96, 1, 1]           \nenc2.5.sca.fc.bias                                 96 [96]                     \nenc2.5.norm.weight                                 96 [96]                     \nenc2.5.norm.bias                                   96 [96]                     \ndown2.weight                                   73,728 [192, 96, 2, 2]          \ndown2.bias                                        192 [192]                    \nenc3.0.conv1.weight                            73,728 [384, 192, 1, 1]         \nenc3.0.conv1.bias                                 384 [384]                    \nenc3.0.conv2.weight                             3,456 [384, 1, 3, 3]           \nenc3.0.conv2.bias                                 384 [384]                    \nenc3.0.conv3.weight                            36,864 [192, 192, 1, 1]         \nenc3.0.conv3.bias                                 192 [192]                    \nenc3.0.sca.fc.weight                           36,864 [192, 192, 1, 1]         \nenc3.0.sca.fc.bias                                192 [192]                    \nenc3.0.norm.weight                                192 [192]                    \nenc3.0.norm.bias                                  192 [192]                    \nenc3.1.conv1.weight                            73,728 [384, 192, 1, 1]         \nenc3.1.conv1.bias                                 384 [384]                    \nenc3.1.conv2.weight                             3,456 [384, 1, 3, 3]           \nenc3.1.conv2.bias                                 384 [384]                    \nenc3.1.conv3.weight                            36,864 [192, 192, 1, 1]         \nenc3.1.conv3.bias                                 192 [192]                    \nenc3.1.sca.fc.weight                           36,864 [192, 192, 1, 1]         \nenc3.1.sca.fc.bias                                192 [192]                    \nenc3.1.norm.weight                                192 [192]                    \nenc3.1.norm.bias                                  192 [192]                    \nenc3.2.conv1.weight                            73,728 [384, 192, 1, 1]         \nenc3.2.conv1.bias                                 384 [384]                    \nenc3.2.conv2.weight                             3,456 [384, 1, 3, 3]           \nenc3.2.conv2.bias                                 384 [384]                    \nenc3.2.conv3.weight                            36,864 [192, 192, 1, 1]         \nenc3.2.conv3.bias                                 192 [192]                    \nenc3.2.sca.fc.weight                           36,864 [192, 192, 1, 1]         \nenc3.2.sca.fc.bias                                192 [192]                    \nenc3.2.norm.weight                                192 [192]                    \nenc3.2.norm.bias                                  192 [192]                    \nenc3.3.conv1.weight                            73,728 [384, 192, 1, 1]         \nenc3.3.conv1.bias                                 384 [384]                    \nenc3.3.conv2.weight                             3,456 [384, 1, 3, 3]           \nenc3.3.conv2.bias                                 384 [384]                    \nenc3.3.conv3.weight                            36,864 [192, 192, 1, 1]         \nenc3.3.conv3.bias                                 192 [192]                    \nenc3.3.sca.fc.weight                           36,864 [192, 192, 1, 1]         \nenc3.3.sca.fc.bias                                192 [192]                    \nenc3.3.norm.weight                                192 [192]                    \nenc3.3.norm.bias                                  192 [192]                    \nenc3.4.conv1.weight                            73,728 [384, 192, 1, 1]         \nenc3.4.conv1.bias                                 384 [384]                    \nenc3.4.conv2.weight                             3,456 [384, 1, 3, 3]           \nenc3.4.conv2.bias                                 384 [384]                    \nenc3.4.conv3.weight                            36,864 [192, 192, 1, 1]         \nenc3.4.conv3.bias                                 192 [192]                    \nenc3.4.sca.fc.weight                           36,864 [192, 192, 1, 1]         \nenc3.4.sca.fc.bias                                192 [192]                    \nenc3.4.norm.weight                                192 [192]                    \nenc3.4.norm.bias                                  192 [192]                    \nenc3.5.conv1.weight                            73,728 [384, 192, 1, 1]         \nenc3.5.conv1.bias                                 384 [384]                    \nenc3.5.conv2.weight                             3,456 [384, 1, 3, 3]           \nenc3.5.conv2.bias                                 384 [384]                    \nenc3.5.conv3.weight                            36,864 [192, 192, 1, 1]         \nenc3.5.conv3.bias                                 192 [192]                    \nenc3.5.sca.fc.weight                           36,864 [192, 192, 1, 1]         \nenc3.5.sca.fc.bias                                192 [192]                    \nenc3.5.norm.weight                                192 [192]                    \nenc3.5.norm.bias                                  192 [192]                    \ndown3.weight                                  294,912 [384, 192, 2, 2]         \ndown3.bias                                        384 [384]                    \nwav_intro.weight                                1,296 [48, 3, 3, 3]            \nwav_intro.bias                                     48 [48]                     \nwav_block1.ll_conv.0.weight                    20,736 [48, 48, 3, 3]           \nwav_block1.ll_conv.0.bias                          48 [48]                     \nwav_block1.ll_conv.2.weight                    20,736 [48, 48, 3, 3]           \nwav_block1.ll_conv.2.bias                          48 [48]                     \nwav_block1.hf_conv.0.weight                    62,208 [144, 48, 3, 3]          \nwav_block1.hf_conv.0.bias                         144 [144]                    \nwav_block1.hf_conv.2.weight                    62,208 [144, 48, 3, 3]          \nwav_block1.hf_conv.2.bias                         144 [144]                    \nwav_block1.hf_attn.1.weight                    20,736 [144, 144, 1, 1]         \nwav_block1.hf_attn.1.bias                         144 [144]                    \nwav_proj1.weight                                4,608 [96, 48, 1, 1]           \nwav_proj1.bias                                     96 [96]                     \nwav_block2.ll_conv.0.weight                    82,944 [96, 96, 3, 3]           \nwav_block2.ll_conv.0.bias                          96 [96]                     \nwav_block2.ll_conv.2.weight                    82,944 [96, 96, 3, 3]           \nwav_block2.ll_conv.2.bias                          96 [96]                     \nwav_block2.hf_conv.0.weight                   248,832 [288, 96, 3, 3]          \nwav_block2.hf_conv.0.bias                         288 [288]                    \nwav_block2.hf_conv.2.weight                   248,832 [288, 96, 3, 3]          \nwav_block2.hf_conv.2.bias                         288 [288]                    \nwav_block2.hf_attn.1.weight                    82,944 [288, 288, 1, 1]         \nwav_block2.hf_attn.1.bias                         288 [288]                    \nwav_proj2.weight                               18,432 [192, 96, 1, 1]          \nwav_proj2.bias                                    192 [192]                    \nwav_block3.ll_conv.0.weight                   331,776 [192, 192, 3, 3]         \nwav_block3.ll_conv.0.bias                         192 [192]                    \nwav_block3.ll_conv.2.weight                   331,776 [192, 192, 3, 3]         \nwav_block3.ll_conv.2.bias                         192 [192]                    \nwav_block3.hf_conv.0.weight                   995,328 [576, 192, 3, 3]         \nwav_block3.hf_conv.0.bias                         576 [576]                    \nwav_block3.hf_conv.2.weight                   995,328 [576, 192, 3, 3]         \nwav_block3.hf_conv.2.bias                         576 [576]                    \nwav_block3.hf_attn.1.weight                   331,776 [576, 576, 1, 1]         \nwav_block3.hf_attn.1.bias                         576 [576]                    \nfusion1.spatial_proj.weight                     9,216 [96, 96, 1, 1]           \nfusion1.spatial_proj.bias                          96 [96]                     \nfusion1.wavelet_proj.weight                     9,216 [96, 96, 1, 1]           \nfusion1.wavelet_proj.bias                          96 [96]                     \nfusion1.gate.0.weight                          18,432 [96, 192, 1, 1]          \nfusion1.gate.0.bias                                96 [96]                     \nfusion1.out.weight                              9,216 [96, 96, 1, 1]           \nfusion1.out.bias                                   96 [96]                     \nfusion2.spatial_proj.weight                    36,864 [192, 192, 1, 1]         \nfusion2.spatial_proj.bias                         192 [192]                    \nfusion2.wavelet_proj.weight                    36,864 [192, 192, 1, 1]         \nfusion2.wavelet_proj.bias                         192 [192]                    \nfusion2.gate.0.weight                          73,728 [192, 384, 1, 1]         \nfusion2.gate.0.bias                               192 [192]                    \nfusion2.out.weight                             36,864 [192, 192, 1, 1]         \nfusion2.out.bias                                  192 [192]                    \nbottleneck.0.conv1.weight                     294,912 [768, 384, 1, 1]         \nbottleneck.0.conv1.bias                           768 [768]                    \nbottleneck.0.conv2.weight                       6,912 [768, 1, 3, 3]           \nbottleneck.0.conv2.bias                           768 [768]                    \nbottleneck.0.conv3.weight                     147,456 [384, 384, 1, 1]         \nbottleneck.0.conv3.bias                           384 [384]                    \nbottleneck.0.sca.fc.weight                    147,456 [384, 384, 1, 1]         \nbottleneck.0.sca.fc.bias                          384 [384]                    \nbottleneck.0.norm.weight                          384 [384]                    \nbottleneck.0.norm.bias                            384 [384]                    \nbottleneck.1.h_conv.weight                      2,688 [384, 1, 1, 7]           \nbottleneck.1.h_conv.bias                          384 [384]                    \nbottleneck.1.v_conv.weight                      2,688 [384, 1, 7, 1]           \nbottleneck.1.v_conv.bias                          384 [384]                    \nbottleneck.1.proj.weight                      294,912 [384, 768, 1, 1]         \nbottleneck.1.proj.bias                            384 [384]                    \nbottleneck.2.conv1.weight                     294,912 [768, 384, 1, 1]         \nbottleneck.2.conv1.bias                           768 [768]                    \nbottleneck.2.conv2.weight                       6,912 [768, 1, 3, 3]           \nbottleneck.2.conv2.bias                           768 [768]                    \nbottleneck.2.conv3.weight                     147,456 [384, 384, 1, 1]         \nbottleneck.2.conv3.bias                           384 [384]                    \nbottleneck.2.sca.fc.weight                    147,456 [384, 384, 1, 1]         \nbottleneck.2.sca.fc.bias                          384 [384]                    \nbottleneck.2.norm.weight                          384 [384]                    \nbottleneck.2.norm.bias                            384 [384]                    \nbottleneck.3.conv1.weight                     294,912 [768, 384, 1, 1]         \nbottleneck.3.conv1.bias                           768 [768]                    \nbottleneck.3.conv2.weight                       6,912 [768, 1, 3, 3]           \nbottleneck.3.conv2.bias                           768 [768]                    \nbottleneck.3.conv3.weight                     147,456 [384, 384, 1, 1]         \nbottleneck.3.conv3.bias                           384 [384]                    \nbottleneck.3.sca.fc.weight                    147,456 [384, 384, 1, 1]         \nbottleneck.3.sca.fc.bias                          384 [384]                    \nbottleneck.3.norm.weight                          384 [384]                    \nbottleneck.3.norm.bias                            384 [384]                    \nup3.weight                                    294,912 [384, 192, 2, 2]         \nup3.bias                                          192 [192]                    \ndec3.0.conv1.weight                            73,728 [384, 192, 1, 1]         \ndec3.0.conv1.bias                                 384 [384]                    \ndec3.0.conv2.weight                             3,456 [384, 1, 3, 3]           \ndec3.0.conv2.bias                                 384 [384]                    \ndec3.0.conv3.weight                            36,864 [192, 192, 1, 1]         \ndec3.0.conv3.bias                                 192 [192]                    \ndec3.0.sca.fc.weight                           36,864 [192, 192, 1, 1]         \ndec3.0.sca.fc.bias                                192 [192]                    \ndec3.0.norm.weight                                192 [192]                    \ndec3.0.norm.bias                                  192 [192]                    \ndec3.1.conv1.weight                            73,728 [384, 192, 1, 1]         \ndec3.1.conv1.bias                                 384 [384]                    \ndec3.1.conv2.weight                             3,456 [384, 1, 3, 3]           \ndec3.1.conv2.bias                                 384 [384]                    \ndec3.1.conv3.weight                            36,864 [192, 192, 1, 1]         \ndec3.1.conv3.bias                                 192 [192]                    \ndec3.1.sca.fc.weight                           36,864 [192, 192, 1, 1]         \ndec3.1.sca.fc.bias                                192 [192]                    \ndec3.1.norm.weight                                192 [192]                    \ndec3.1.norm.bias                                  192 [192]                    \ndec3.2.conv1.weight                            73,728 [384, 192, 1, 1]         \ndec3.2.conv1.bias                                 384 [384]                    \ndec3.2.conv2.weight                             3,456 [384, 1, 3, 3]           \ndec3.2.conv2.bias                                 384 [384]                    \ndec3.2.conv3.weight                            36,864 [192, 192, 1, 1]         \ndec3.2.conv3.bias                                 192 [192]                    \ndec3.2.sca.fc.weight                           36,864 [192, 192, 1, 1]         \ndec3.2.sca.fc.bias                                192 [192]                    \ndec3.2.norm.weight                                192 [192]                    \ndec3.2.norm.bias                                  192 [192]                    \ndec3.3.conv1.weight                            73,728 [384, 192, 1, 1]         \ndec3.3.conv1.bias                                 384 [384]                    \ndec3.3.conv2.weight                             3,456 [384, 1, 3, 3]           \ndec3.3.conv2.bias                                 384 [384]                    \ndec3.3.conv3.weight                            36,864 [192, 192, 1, 1]         \ndec3.3.conv3.bias                                 192 [192]                    \ndec3.3.sca.fc.weight                           36,864 [192, 192, 1, 1]         \ndec3.3.sca.fc.bias                                192 [192]                    \ndec3.3.norm.weight                                192 [192]                    \ndec3.3.norm.bias                                  192 [192]                    \ndec3.4.conv1.weight                            73,728 [384, 192, 1, 1]         \ndec3.4.conv1.bias                                 384 [384]                    \ndec3.4.conv2.weight                             3,456 [384, 1, 3, 3]           \ndec3.4.conv2.bias                                 384 [384]                    \ndec3.4.conv3.weight                            36,864 [192, 192, 1, 1]         \ndec3.4.conv3.bias                                 192 [192]                    \ndec3.4.sca.fc.weight                           36,864 [192, 192, 1, 1]         \ndec3.4.sca.fc.bias                                192 [192]                    \ndec3.4.norm.weight                                192 [192]                    \ndec3.4.norm.bias                                  192 [192]                    \ndec3.5.conv1.weight                            73,728 [384, 192, 1, 1]         \ndec3.5.conv1.bias                                 384 [384]                    \ndec3.5.conv2.weight                             3,456 [384, 1, 3, 3]           \ndec3.5.conv2.bias                                 384 [384]                    \ndec3.5.conv3.weight                            36,864 [192, 192, 1, 1]         \ndec3.5.conv3.bias                                 192 [192]                    \ndec3.5.sca.fc.weight                           36,864 [192, 192, 1, 1]         \ndec3.5.sca.fc.bias                                192 [192]                    \ndec3.5.norm.weight                                192 [192]                    \ndec3.5.norm.bias                                  192 [192]                    \nup2.weight                                     73,728 [192, 96, 2, 2]          \nup2.bias                                           96 [96]                     \ndec2.0.conv1.weight                            18,432 [192, 96, 1, 1]          \ndec2.0.conv1.bias                                 192 [192]                    \ndec2.0.conv2.weight                             1,728 [192, 1, 3, 3]           \ndec2.0.conv2.bias                                 192 [192]                    \ndec2.0.conv3.weight                             9,216 [96, 96, 1, 1]           \ndec2.0.conv3.bias                                  96 [96]                     \ndec2.0.sca.fc.weight                            9,216 [96, 96, 1, 1]           \ndec2.0.sca.fc.bias                                 96 [96]                     \ndec2.0.norm.weight                                 96 [96]                     \ndec2.0.norm.bias                                   96 [96]                     \ndec2.1.conv1.weight                            18,432 [192, 96, 1, 1]          \ndec2.1.conv1.bias                                 192 [192]                    \ndec2.1.conv2.weight                             1,728 [192, 1, 3, 3]           \ndec2.1.conv2.bias                                 192 [192]                    \ndec2.1.conv3.weight                             9,216 [96, 96, 1, 1]           \ndec2.1.conv3.bias                                  96 [96]                     \ndec2.1.sca.fc.weight                            9,216 [96, 96, 1, 1]           \ndec2.1.sca.fc.bias                                 96 [96]                     \ndec2.1.norm.weight                                 96 [96]                     \ndec2.1.norm.bias                                   96 [96]                     \ndec2.2.conv1.weight                            18,432 [192, 96, 1, 1]          \ndec2.2.conv1.bias                                 192 [192]                    \ndec2.2.conv2.weight                             1,728 [192, 1, 3, 3]           \ndec2.2.conv2.bias                                 192 [192]                    \ndec2.2.conv3.weight                             9,216 [96, 96, 1, 1]           \ndec2.2.conv3.bias                                  96 [96]                     \ndec2.2.sca.fc.weight                            9,216 [96, 96, 1, 1]           \ndec2.2.sca.fc.bias                                 96 [96]                     \ndec2.2.norm.weight                                 96 [96]                     \ndec2.2.norm.bias                                   96 [96]                     \ndec2.3.conv1.weight                            18,432 [192, 96, 1, 1]          \ndec2.3.conv1.bias                                 192 [192]                    \ndec2.3.conv2.weight                             1,728 [192, 1, 3, 3]           \ndec2.3.conv2.bias                                 192 [192]                    \ndec2.3.conv3.weight                             9,216 [96, 96, 1, 1]           \ndec2.3.conv3.bias                                  96 [96]                     \ndec2.3.sca.fc.weight                            9,216 [96, 96, 1, 1]           \ndec2.3.sca.fc.bias                                 96 [96]                     \ndec2.3.norm.weight                                 96 [96]                     \ndec2.3.norm.bias                                   96 [96]                     \ndec2.4.conv1.weight                            18,432 [192, 96, 1, 1]          \ndec2.4.conv1.bias                                 192 [192]                    \ndec2.4.conv2.weight                             1,728 [192, 1, 3, 3]           \ndec2.4.conv2.bias                                 192 [192]                    \ndec2.4.conv3.weight                             9,216 [96, 96, 1, 1]           \ndec2.4.conv3.bias                                  96 [96]                     \ndec2.4.sca.fc.weight                            9,216 [96, 96, 1, 1]           \ndec2.4.sca.fc.bias                                 96 [96]                     \ndec2.4.norm.weight                                 96 [96]                     \ndec2.4.norm.bias                                   96 [96]                     \ndec2.5.conv1.weight                            18,432 [192, 96, 1, 1]          \ndec2.5.conv1.bias                                 192 [192]                    \ndec2.5.conv2.weight                             1,728 [192, 1, 3, 3]           \ndec2.5.conv2.bias                                 192 [192]                    \ndec2.5.conv3.weight                             9,216 [96, 96, 1, 1]           \ndec2.5.conv3.bias                                  96 [96]                     \ndec2.5.sca.fc.weight                            9,216 [96, 96, 1, 1]           \ndec2.5.sca.fc.bias                                 96 [96]                     \ndec2.5.norm.weight                                 96 [96]                     \ndec2.5.norm.bias                                   96 [96]                     \nup1.weight                                     18,432 [96, 48, 2, 2]           \nup1.bias                                           48 [48]                     \ndec1.0.conv1.weight                             4,608 [96, 48, 1, 1]           \ndec1.0.conv1.bias                                  96 [96]                     \ndec1.0.conv2.weight                               864 [96, 1, 3, 3]            \ndec1.0.conv2.bias                                  96 [96]                     \ndec1.0.conv3.weight                             2,304 [48, 48, 1, 1]           \ndec1.0.conv3.bias                                  48 [48]                     \ndec1.0.sca.fc.weight                            2,304 [48, 48, 1, 1]           \ndec1.0.sca.fc.bias                                 48 [48]                     \ndec1.0.norm.weight                                 48 [48]                     \ndec1.0.norm.bias                                   48 [48]                     \ndec1.1.conv1.weight                             4,608 [96, 48, 1, 1]           \ndec1.1.conv1.bias                                  96 [96]                     \ndec1.1.conv2.weight                               864 [96, 1, 3, 3]            \ndec1.1.conv2.bias                                  96 [96]                     \ndec1.1.conv3.weight                             2,304 [48, 48, 1, 1]           \ndec1.1.conv3.bias                                  48 [48]                     \ndec1.1.sca.fc.weight                            2,304 [48, 48, 1, 1]           \ndec1.1.sca.fc.bias                                 48 [48]                     \ndec1.1.norm.weight                                 48 [48]                     \ndec1.1.norm.bias                                   48 [48]                     \ndec1.2.conv1.weight                             4,608 [96, 48, 1, 1]           \ndec1.2.conv1.bias                                  96 [96]                     \ndec1.2.conv2.weight                               864 [96, 1, 3, 3]            \ndec1.2.conv2.bias                                  96 [96]                     \ndec1.2.conv3.weight                             2,304 [48, 48, 1, 1]           \ndec1.2.conv3.bias                                  48 [48]                     \ndec1.2.sca.fc.weight                            2,304 [48, 48, 1, 1]           \ndec1.2.sca.fc.bias                                 48 [48]                     \ndec1.2.norm.weight                                 48 [48]                     \ndec1.2.norm.bias                                   48 [48]                     \ndec1.3.conv1.weight                             4,608 [96, 48, 1, 1]           \ndec1.3.conv1.bias                                  96 [96]                     \ndec1.3.conv2.weight                               864 [96, 1, 3, 3]            \ndec1.3.conv2.bias                                  96 [96]                     \ndec1.3.conv3.weight                             2,304 [48, 48, 1, 1]           \ndec1.3.conv3.bias                                  48 [48]                     \ndec1.3.sca.fc.weight                            2,304 [48, 48, 1, 1]           \ndec1.3.sca.fc.bias                                 48 [48]                     \ndec1.3.norm.weight                                 48 [48]                     \ndec1.3.norm.bias                                   48 [48]                     \nrefine.0.weight                                20,736 [48, 48, 3, 3]           \nrefine.0.bias                                      48 [48]                     \nrefine.2.weight                                20,736 [48, 48, 3, 3]           \nrefine.2.bias                                      48 [48]                     \noutro.weight                                    1,296 [3, 48, 3, 3]            \noutro.bias                                          3 [3]                      \n--------------------------------------------------------------------------------\nTotal Parameters                            9,484,659\nTrainable Parameters                        9,484,659\nTotal (Millions)                                 9.48M\n================================================================================\n\nARCHITECTURE SUMMARY:\n  - Base Channels: 48\n  - Encoder Blocks: [4, 6, 6, 4]\n  - Dual-Branch: Spatial (NAFBlocks) + Wavelet (DWT)\n  - Fusion: Cross-Branch Gated Fusion at 2 levels\n  - Bottleneck: Strip Attention\n================================================================================\n\nUsing 2 GPUs\nDownloading: \"https://download.pytorch.org/models/vgg19-dcbb9e9d.pth\" to /root/.cache/torch/hub/checkpoints/vgg19-dcbb9e9d.pth\n","output_type":"stream"},{"name":"stderr","text":"100%|| 548M/548M [00:02<00:00, 239MB/s]  \n","output_type":"stream"},{"name":"stdout","text":"Found 8422 train pairs in HIDE\nFound 4050 test pairs in HIDE\n\n=== Baseline Check ===\nBaseline PSNR (blur vs sharp): 23.43 dB\n\n=== Starting Training ===\n","output_type":"stream"},{"name":"stderr","text":"Epoch 1/30: 100%|| 2105/2105 [14:17<00:00,  2.45it/s, loss=1.2794, lr=2.00e-04]\nValidating: 100%|| 4050/4050 [36:17<00:00,  1.86it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 1: Loss=0.7106, PSNR=24.73dB\n  Components - L1:0.0420, VGG:1.6224, FFT:9.6900, Grad:0.2109, Wav:0.0401\n  *** New best model saved! PSNR: 24.73dB ***\n","output_type":"stream"},{"name":"stderr","text":"Epoch 2/30: 100%|| 2105/2105 [13:57<00:00,  2.51it/s, loss=0.6357, lr=1.99e-04]\nEpoch 3/30: 100%|| 2105/2105 [13:56<00:00,  2.52it/s, loss=0.4432, lr=1.98e-04]\nEpoch 4/30: 100%|| 2105/2105 [13:58<00:00,  2.51it/s, loss=0.5784, lr=1.95e-04]\nEpoch 5/30: 100%|| 2105/2105 [13:59<00:00,  2.51it/s, loss=0.7588, lr=1.91e-04]\nEpoch 6/30: 100%|| 2105/2105 [14:05<00:00,  2.49it/s, loss=1.1327, lr=1.87e-04]\nEpoch 7/30: 100%|| 2105/2105 [14:06<00:00,  2.49it/s, loss=0.4192, lr=1.81e-04]\nEpoch 8/30: 100%|| 2105/2105 [14:04<00:00,  2.49it/s, loss=0.5984, lr=1.74e-04]\nEpoch 9/30: 100%|| 2105/2105 [14:01<00:00,  2.50it/s, loss=0.6778, lr=1.67e-04]\nEpoch 10/30: 100%|| 2105/2105 [14:03<00:00,  2.50it/s, loss=0.5509, lr=1.59e-04]\nValidating: 100%|| 4050/4050 [36:19<00:00,  1.86it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 10: Loss=0.6026, PSNR=27.50dB\n  Components - L1:0.0293, VGG:1.3996, FFT:8.3014, Grad:0.1756, Wav:0.0355\n  *** New best model saved! PSNR: 27.50dB ***\n","output_type":"stream"},{"name":"stderr","text":"Epoch 11/30: 100%|| 2105/2105 [14:03<00:00,  2.50it/s, loss=0.7635, lr=1.50e-04]\nEpoch 12/30: 100%|| 2105/2105 [13:59<00:00,  2.51it/s, loss=0.6433, lr=1.41e-04]\nEpoch 13/30: 100%|| 2105/2105 [14:01<00:00,  2.50it/s, loss=0.5943, lr=1.31e-04]\nEpoch 14/30: 100%|| 2105/2105 [13:59<00:00,  2.51it/s, loss=0.5081, lr=1.21e-04]\nEpoch 15/30: 100%|| 2105/2105 [14:01<00:00,  2.50it/s, loss=0.4639, lr=1.10e-04]\nEpoch 16/30: 100%|| 2105/2105 [14:00<00:00,  2.50it/s, loss=0.5775, lr=1.00e-04]\nEpoch 17/30: 100%|| 2105/2105 [14:02<00:00,  2.50it/s, loss=0.5522, lr=8.96e-05]\nEpoch 18/30: 100%|| 2105/2105 [13:58<00:00,  2.51it/s, loss=0.3833, lr=7.93e-05]\nEpoch 19/30: 100%|| 2105/2105 [14:00<00:00,  2.50it/s, loss=0.7211, lr=6.92e-05]\nEpoch 20/30: 100%|| 2105/2105 [13:57<00:00,  2.51it/s, loss=0.4699, lr=5.94e-05]\nValidating: 100%|| 4050/4050 [36:18<00:00,  1.86it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 20: Loss=0.5612, PSNR=28.33dB\n  Components - L1:0.0260, VGG:1.3225, FFT:7.7253, Grad:0.1605, Wav:0.0334\n  *** New best model saved! PSNR: 28.33dB ***\n","output_type":"stream"},{"name":"stderr","text":"Epoch 21/30: 100%|| 2105/2105 [13:56<00:00,  2.52it/s, loss=0.4877, lr=5.01e-05]\nEpoch 22/30: 100%|| 2105/2105 [13:55<00:00,  2.52it/s, loss=0.5226, lr=4.13e-05]\nEpoch 23/30: 100%|| 2105/2105 [13:56<00:00,  2.52it/s, loss=0.4784, lr=3.32e-05]\nEpoch 24/30: 100%|| 2105/2105 [13:56<00:00,  2.52it/s, loss=0.8744, lr=2.58e-05]\nEpoch 25/30: 100%|| 2105/2105 [13:58<00:00,  2.51it/s, loss=0.3664, lr=1.92e-05]\nEpoch 26/30: 100%|| 2105/2105 [13:59<00:00,  2.51it/s, loss=0.4916, lr=1.35e-05]\nEpoch 27/30: 100%|| 2105/2105 [13:59<00:00,  2.51it/s, loss=0.9436, lr=8.74e-06]\nEpoch 28/30: 100%|| 2105/2105 [13:59<00:00,  2.51it/s, loss=0.4650, lr=4.99e-06]\nEpoch 29/30: 100%|| 2105/2105 [14:01<00:00,  2.50it/s, loss=0.5247, lr=2.28e-06]\nEpoch 30/30: 100%|| 2105/2105 [14:03<00:00,  2.50it/s, loss=0.7571, lr=6.48e-07]\nValidating: 100%|| 4050/4050 [36:19<00:00,  1.86it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 30: Loss=0.5545, PSNR=28.61dB\n  Components - L1:0.0253, VGG:1.3141, FFT:7.6247, Grad:0.1589, Wav:0.0334\n  *** New best model saved! PSNR: 28.61dB ***\n\n=== Training Complete ===\nBest PSNR: 28.61dB\nSaved 8 comparisons to /kaggle/working/results_hide; mean PSNR=28.35dB\n","output_type":"stream"}],"execution_count":1}]}